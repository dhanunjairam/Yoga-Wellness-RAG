# RAG API (Domain-Agnostic)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.115.0-brightgreen.svg)](https://fastapi.tiangolo.com)
[![Python](https://img.shields.io/badge/Python-3.11%2B-blue.svg)](https://python.org)
[![Pinecone](https://img.shields.io/badge/Pinecone-5.1.0-orange.svg)](https://pinecone.io)
[![RAGAS](https://img.shields.io/badge/RAGAS-0.1.9-purple.svg)](https://github.com/explodinggradients/ragas)
[![UV](https://img.shields.io/badge/UV-0.4.18-brightgreen)](https://astral.sh/uv)

A production-ready **Retrieval-Augmented Generation (RAG)** system that works with **any PDF documents** - not limited to any specific domain.

## ğŸš€ Features
[![FastAPI](https://img.shields.io/badge/-REST_API-blue)](https://fastapi.tiangolo.com)
[![Pinecone](https://img.shields.io/badge/-Vector_DB-orange)](https://pinecone.io)
[![RAGAS](https://img.shields.io/badge/-Eval_Framework-purple)](https://github.com/explodinggradients/ragas)

- **Full RAG Pipeline**: PDF â†’ Markdown â†’ Vector Store â†’ LLM â†’ RAGAS Evaluation
- **Pinecone Vector Search** with hybrid retrieval & MMR diversity
- **Configurable LLM** with strict "context-only" prompting
- **RAGAS Metrics** for retrieval & generation quality (async evaluation)
- **FastAPI** with `/chat` (full pipeline) + `/index-status` endpoints
- **Automatic PDF Processing** - converts only when needed

## ğŸ—ï¸ Architecture

PDFs â†’ [data_source/] â†’ Markdown Pipeline â†’ [markdown_data_sources/] â†’ Chunking â†’ Pinecone
â†“
User Query â†’ Retriever (Hybrid Search + MMR) â†’ Chatbot (Context-Only LLM) â†’ RAGAS Eval

## ğŸ“ Project Structure

â”œâ”€â”€ main.py # FastAPI app + endpoints

â”œâ”€â”€ README.md

â”œâ”€â”€ pyproject.toml # Dependencies

â”œâ”€â”€ env_example.txt # Env vars template

â”œâ”€â”€ uv.lock # UV lockfile

â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ data/
â”‚ â”‚ â”œâ”€â”€ markdown_data_pipeline.py # PDF â†’ Markdown conversion
â”‚ â”‚ â”œâ”€â”€ data_source/ # ğŸ“¥ INPUT: Raw PDFs here
â”‚ â”‚ â””â”€â”€ markdown_data_sources/ # ğŸ“¤ OUTPUT: Generated .md files
â”‚ â”œâ”€â”€ data_processing/
â”‚ â”‚ â”œâ”€â”€ data_chunking_loading.py # Markdown â†’ Pinecone ingestion
â”‚ â”‚ â””â”€â”€ check_pincone_index.py # List indexed files
â”‚ â”œâ”€â”€ data_retriver/
â”‚ â”‚ â””â”€â”€ data_pinecone_retriver.py # Hybrid search + MMR + metrics
â”‚ â”œâ”€â”€ llm/
â”‚ â”‚ â””â”€â”€ llm_file.py # Configurable ChatOpenAI + strict prompting
â”‚ â””â”€â”€ evaluation/
â”‚ â””â”€â”€ ragas_evaluation.py # Async RAGAS metrics (5 core metrics)


## âš™ï¸ Quick Start with UV

### 1. **Install UV** (if not installed)
```bash
# macOS/Linux
curl -LsSf https://astral.sh/uv/install.sh | sh

# Windows
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"


2. Setup Environment
# Clone & enter project
git clone <repo> && cd rag-api
uv sync          # Installs all dependencies from pyproject.toml

# Copy & configure environment
cp env_example.txt .env
# Edit .env with your keys:
# PINECONE_API=your_pinecone_key
# _API_KEY=your_llm_key (Perplexity/OpenAI/etc)
# INDEX_NAME=your_pinecone_index

3. Add Your PDFs

src/data/data_source/
â””â”€â”€ your_document_1.pdf
â””â”€â”€ your_document_2.pdf
â””â”€â”€ any_topic.pdf


4. Run API

bash
# Development (auto-reload)
uvicorn main:app --reload --port 8000

# Production
uvicorn main:app --host 0.0.0.0 --port 8000 --workers 4

ğŸŒ API Endpoints
POST /chat

Full RAG Pipeline - Retrieve â†’ Generate â†’ Evaluate

bash
curl -X POST "http://localhost:8000/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "your question here",
    "top_k": 7,
    "score_threshold": 0.7,
    "use_mmr": true
  }'

Sample Response:

json
{
  "success": true,
  "data": {
    "answer": "Answer from your documents... [Source.pdf | Section]",
    "retrieval_metrics": {"precision_at_k": 0.857, "latency_ms": 23.4},
    "ragas_scores": {"context_precision": 0.89, "faithfulness": 0.94},
    "evaluation_summary": {"overall_score": 0.823, "status": "ğŸŸ¢ PASS"},
    "sources": [...],
    "indexed_files": {"doc1.pdf": 45, "doc2.pdf": 32}
  }
}

GET /index-status

Vector Store Health Check

curl http://localhost:8000/index-status

GET /health

API Health Check

curl http://localhost:8000/health

ğŸ”§ Environment Variables

Create .env from env_example.txt:

bash
# Required
PINECONE_API=your_pinecone_key
_API_KEY=your_llm_key
INDEX_NAME=your_pinecone_index

# Optional Tuning
EMBEDDING_MODEL=nomic-ai/nomic-embed-text-v1.5
LLM_MODEL=your_model_name
LLM_BASE_URL=your_llm_base_url  # Perplexity/OpenAI/etc
OLLAMA_BASE_URL=http://localhost:11434  # Local embeddings (optional)


ğŸ“ˆ Quality Metrics

Retrieval Metrics:

    precision_at_k: Relevant chunks ranked higher

    source_diversity: Multi-file coverage

    latency_ms: End-to-end speed

RAGAS Metrics:

    Context Precision/Recall/Relevancy

    Faithfulness + Answer Relevancy

ğŸ”„ Auto Data Pipeline

    On Startup: Auto-checks data_source/*.pdf vs markdown_data_sources/*.md

    Conversion: Only processes new/changed PDFs â†’ Markdown

    Ingestion: Chunks â†’ Pinecone vectors

    Status: /index-status shows indexed files + chunk counts


ğŸŒ Domain Agnostic

Works with any PDF documents:

src/data/data_source/
â”œâ”€â”€ legal_contracts.pdf     â†’ Legal RAG
â”œâ”€â”€ medical_papers.pdf      â†’ Medical RAG
â”œâ”€â”€ technical_docs.pdf      â†’ Technical RAG
â””â”€â”€ any_content.pdf         â†’ Any RAG

Just drop PDFs and query!
ğŸ› ï¸ UV Development Workflow

# Clean environment
uv cache clean
uv sync --dev

# Run with auto-reload
uv run uvicorn main:app --reload

# Lint & Format
uv tool install ruff
uv run ruff check . && uv run ruff format .

# Shell with dependencies
uv run -- python  # Opens Python REPL with all deps

# Add new dependency
uv add requests
uv sync